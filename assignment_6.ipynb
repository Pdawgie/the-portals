{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"assignment_6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "heading",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# STK 353\n",
    "\n",
    "## Assignment 6: Machine Learning\n",
    "\n",
    "#### Internal examiner: Ineke Derks\n",
    "#### External examiner: Dr Sebnem Er\n",
    "\n",
    "### Total points: 35\n",
    "\n",
    "- Submission deadline: 23:00, Thursday 10 November 2021.\n",
    "- This assignment is individual work.\n",
    "- Some of the tests are hidden, and some are visible in order to guide you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Prepare the data\n",
    "\n",
    "The data can be found in 'IMDB.csv'. The IMDB dataset has 50K movie reviews, which can be used for natural language processing or text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. In this assignment, you are required to predict the number of positive and negative reviews using logistic regression and naive Bayes. Furthermore, you are required to determine the underlying topics using latent Dirichlet allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.1 [1]\n",
    "\n",
    "- Read the data into a dataframe and call it 'review'.\n",
    "- **Important**: Make sure the dataset is saved in the **same** directory as your notebook.\n",
    "- Write the review column to a list. Only use the first 2500 rows. Call this list 'X_arr'.\n",
    "\n",
    "**Note**: Pay attention to the variable names specified. It is important that you keep to the names specified.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.1\n",
    "points: \n",
    "    each: 0.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "review = pd.read_csv('IMDB.csv')\n",
    "print(review.head())\n",
    "review_col_list = review['review'].tolist()\n",
    "X_arr=review_col_list[0:2500]\n",
    "print(len(X_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1.1 results: All test cases passed!"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.2 [1]\n",
    "\n",
    "- Use sklearn.preprocessing LabelEncoder to encode the labels to binary values 0 or 1. Make a new column in the dataframe for this encoding and call it 'target'. Positive sentiment will be 1 and a negative sentiment will be 0.\n",
    "- Drop the sentiment column.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.2\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  target\n",
      "0  One of the other reviewers has mentioned that ...       1\n",
      "1  A wonderful little production. <br /><br />The...       1\n",
      "2  I thought this was a wonderful way to spend ti...       1\n",
      "3  Basically there's a family where a little boy ...       0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...       1\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "review['target'] = le.fit_transform(review.sentiment)\n",
    "review.drop('sentiment',axis=1,inplace=True)\n",
    "\n",
    "print(review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1.2 results: All test cases passed!"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie review sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 -  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1 [3]\n",
    "\n",
    "- Use the sklearn TfidfVectorizer. Use the following parameters:\n",
    "    - stop_words = 'english'.\n",
    "    - token_pattern = r'\\b[^\\d\\W]+\\b'. \n",
    "- Call the instantiation 'tfidf'.\n",
    "- Get the vectorization (call it bag_of_words) and the feature names (call it 'feature_names_lr').\n",
    "- Write the vectorization to a dataframe with the feature_names as column headings. Call the dataframe 'vectorized_text_lr'.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.1\n",
    "points: \n",
    "    each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "x_arr_other=np.array(X_arr)\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', token_pattern =r'\\b[^\\d\\W]+\\b' )\n",
    "my_tfidf = tfidf.fit_transform(X_arr) #is array\n",
    "my_tfidf1 = my_tfidf.toarray()\n",
    "bag_of_words = pd.DataFrame(my_tfidf1)\n",
    "feature_names_lr =tfidf.get_feature_names()\n",
    "bag_of_words.columns = feature_names_lr\n",
    "vectorized_text_lr=bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.1 results: All test cases passed!"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q_4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Question 2.2 [1]\n",
    "\n",
    "Split the data into a training and test set.\n",
    "\n",
    "- Use a 70/30 split.\n",
    "- Use a seed of 27.\n",
    "- Name the training and test sets: X_train, Y_train, X_test, Y_test.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2\n",
    "points: \n",
    "    each: 0.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_4_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(vectorized_text_lr,test_size=0.30,random_state=27)\n",
    "\n",
    "X_train = train\n",
    "Y_train = review['target'][train.index]\n",
    "\n",
    "X_test = test\n",
    "Y_test = review['target'][test.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.2 results: All test cases passed!"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.3a [1]\n",
    "\n",
    "- Use the training set to check the class imbalance of the data set and call it 'class_ratio'.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.3a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    51.028571\n",
      "0    48.971429\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class_ratio = Y_train.value_counts(normalize=True)*100\n",
    "print(class_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.3a</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.3a results: All test cases passed!"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2.3b [1]\n",
    "\n",
    "- In the markdown cell below, comment on the class ratio.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.3b\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The values from class_ratio of : 51.028571 for 1 and 48.971429 for 0 tells us that there is Slight Imbalance in the classes.\n",
    "\n",
    "A slight imbalance is often not a concern, and the problem can often be treated like a normal classification predictive modeling problem. Therefore the model is still valid and can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Question 2.4  [1]\n",
    "\n",
    "Create an instance of the logistic regression model. Specify solver = 'lbfgs'.\n",
    "\n",
    "- Fit the model to the training data and call it 'logistic_model'. \n",
    "- Calculate the accuracy on the training data and call it 'log_acc'.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train logistic regression\n",
    "logistic_model = LogisticRegression(solver = 'lbfgs', random_state = 42)\n",
    "logistic_model.fit(X_train,Y_train)\n",
    "\n",
    "# Predict\n",
    "predicted = logistic_model.predict(X_test)\n",
    "\n",
    "# # Performance metrics\n",
    "# conf_matrix = confusion_matrix(Y_test,predicted)\n",
    "# print(conf_matrix)\n",
    "\n",
    "log_acc = round(metrics.accuracy_score(Y_test,predicted),4)\n",
    "\n",
    "print(log_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.4 results: All test cases passed!"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.5 [3]\n",
    "\n",
    "Predict class labels for the test set and calculate the class probabilities.\n",
    "\n",
    "- Call the predicted class labels 'predicted'.\n",
    "- Call the predicted class probabilities 'probs'.\n",
    "\n",
    "Calculate the following,\n",
    "\n",
    "- Calculate the accuracy score on the test set and call it 'log_acc_score'.\n",
    "- Calculate the precision score on the test set and call it 'log_prec_score'.\n",
    "- Calculate the recall score on the test set and call it 'log_rec_score'.\n",
    "- Calculate the auc (area under the curve) score on the test set and call it 'log_auc_score'.\n",
    "\n",
    "**Note**: Round the 'log_acc_score', 'log_prec_score', 'log_rec_score', and 'log_auc_score' values to 4 decimal points.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.5\n",
    "points: \n",
    "    each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347 0.8767 0.8074 0.9137\n"
     ]
    }
   ],
   "source": [
    "predicted = logistic_model.predict(X_test)\n",
    "y_pred_proba = logistic_model.predict_proba(X_test)[::,1]\n",
    "\n",
    "log_acc_score=round(metrics.accuracy_score(Y_test, predicted),4)\n",
    "log_rec_score=round(metrics.recall_score(Y_test, predicted),4)\n",
    "log_prec_score=round(metrics.precision_score(Y_test, predicted),4)\n",
    "\n",
    "log_auc_score=round(metrics.roc_auc_score(Y_test, y_pred_proba),4)\n",
    "\n",
    "print(log_acc_score ,log_rec_score,log_prec_score, log_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.5 results: All test cases passed!"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "q_9_1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2.6 [1]\n",
    "\n",
    "Does the model perform better than randomly assigning classes? Explain why.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.6\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hight accuracy score of 0.8347 tells us that: the number of correct predictions/total number of predictons is quite accurate. So, therefore the model made correct predictions 83.37% of the time. \n",
    "\n",
    "heh precision metric also tells us that the modelis correct about 80.74% of the time.\n",
    "\n",
    "The recall metric of 0.8767 tells us that the model was able to identify the data very well.\n",
    "\n",
    "The high AUC value of 0.9137 , tells us that the model did well at distinguishing between the positive and negative classes.\n",
    "\n",
    "All these metrics tells us that the model is functioning deterministically according to the data, and it was not up to random chance.So, yes the model performed better than randomly assigning classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2.7 [3]\n",
    "\n",
    "Draw the ROC curve.\n",
    "\n",
    "- Provide a title, x and y label.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7\n",
    "manual: true\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_proba = logistic_model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# The more that the curve hugs the top left corner of the plot, the better the model does at classifying the data into categories.\n",
    "#So, this model did well in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 3 -  Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3.1 [3]\n",
    "\n",
    "- Use the sklearn Countvectorizer. Use the following parameters:\n",
    "    - stop_words = 'english'.\n",
    "    - token_pattern = r'\\b[^\\d\\W]+\\b'. \n",
    "- Call the instantiation 'matrix'.\n",
    "- Get the vectorization (call it bag_of_words) and the feature names (call it 'feature_names_nb').\n",
    "- Write the vectorization to a dataframe with the feature_names as column headings. Call the dataframe 'vectorized_text_nb'.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1\n",
    "points: \n",
    "    each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix = CountVectorizer(stop_words = 'english',token_pattern = r'\\b[^\\d\\W]+\\b')\n",
    "bag_of_words = matrix.fit_transform(X_arr)\n",
    "feature_names = matrix.get_feature_names()\n",
    "vectorized_text_nb = pd.DataFrame(bag_of_words.todense(),columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3.1 results: All test cases passed!"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3.2 [1]\n",
    "\n",
    "Split the data into a training and test set.\n",
    "\n",
    "- Use a 70/30 split.\n",
    "- Use a seed of 27.\n",
    "- Name the training and test sets: X_train, Y_train, X_test, Y_test.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2\n",
    "points: \n",
    "    each: 0.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(vectorized_text_nb,test_size=0.30,random_state=27)\n",
    "\n",
    "X_train = train\n",
    "Y_train = review['target'][train.index]\n",
    "\n",
    "X_test = test\n",
    "Y_test = review['target'][test.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3.2 results: All test cases passed!"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3.3 [1]\n",
    "\n",
    "Create an instance of the Naive Bayes model.\n",
    "\n",
    "- Use MultinomialNB.\n",
    "- Fit the model to the training data and call it 'nb_model'. \n",
    "- Calculate the accuracy on the training data and call it 'nb_acc'.\n",
    "\n",
    "**Note**: Round the 'nb_acc' value to 4 decimal points.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(X_train,Y_train)\n",
    "predict = nb_model.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(Y_test,predict)\n",
    "\n",
    "nb_acc =round(metrics.accuracy_score(Y_test,predict),4)\n",
    "\n",
    "print(nb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3.3 results: All test cases passed!"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3.4 [3]\n",
    "\n",
    "Predict class labels for the test set and calculate the class probabilities.\n",
    "\n",
    "- Call the predicted class labels 'predicted'.\n",
    "\n",
    "Calculate the following,\n",
    "\n",
    "- Calculate the accuracy score on the test set and call it 'nb_acc_score'.\n",
    "- Calculate the precision score on the test set and call it 'nb_prec_score'.\n",
    "- Calculate the recall score on the test set and call it 'nb_rec_score'.\n",
    "\n",
    "**Note**: Round the 'nb_acc_score', 'nb_prec_score' and 'nb_rec_score' values to 4 decimal points.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.4\n",
    "points: \n",
    "    each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526 0.7909 0.828\n"
     ]
    }
   ],
   "source": [
    "predicted=nb_model.predict(X_test)\n",
    "\n",
    "nb_acc_score=round(metrics.accuracy_score(Y_test, predicted),4)\n",
    "nb_rec_score=round(metrics.recall_score(Y_test, predicted),4)\n",
    "nb_prec_score=round(metrics.precision_score(Y_test, predicted),4)\n",
    "print(nb_prec_score , nb_rec_score , nb_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3.4 results: All test cases passed!"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 3.5 [3]\n",
    "\n",
    "Make use of the performance measures calculated and comment on the model performance.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5\n",
    "manual: true\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hight accuracy score of 0.8280 tells us that: the number of correct predictions/total number of predictons is quite accurate. So, therefore the model made correct predictions 82.80% of the time.\n",
    "\n",
    "The precision metric also tells us that the modelis correct about 85.26% of the time.\n",
    "\n",
    "The recall metric of 0.7909 tells us that the model was able to identify the relevant data very well.\n",
    "\n",
    "All these metrics tells us that the model is functioning deterministically according to the data, and it was not up to random chance.So, yes the model performed better than randomly assigning classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4 [3]\n",
    "\n",
    "Which model do you recommend for classifying the IMDB review data? Give a motivation.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "manual: true\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recomnd the logistic regression model because all its preformance metrics were sliightly higher than the Naive Bayes Classifier model, it was also less computationally intensive and there are more preformace metrics and diagnostics like the AUC and ROC to decern if the model worked well or if it was just a random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Movie review: Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 5.1 [1]\n",
    "\n",
    "- Use the sklearn Countvectorizer. Use the following parameters:\n",
    "    - stop_words = 'english'.\n",
    "    - max_features = 1000.\n",
    "    - token_pattern = r'\\b[^\\d\\W]+\\b'. \n",
    "- Call the instantiation 'lda_mat'.\n",
    "- Get the vectorization (call it bag_of_words) and the feature names (call it 'feature_names')\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5.1\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mat = CountVectorizer(stop_words = 'english', token_pattern =r'\\b[^\\d\\W]+\\b', max_features = 1000)\n",
    "bag_of_words = lda_mat.fit_transform(X_arr)\n",
    "feature_names = lda_mat.get_feature_names()\n",
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components = no_topics,\n",
    "                                max_iter = 5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0).fit(bag_of_words)\n",
    "\n",
    "# print(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5.1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5.1 results: All test cases passed!"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 5.2 [2]\n",
    "\n",
    "Write a function to find the top 15 unique words.\n",
    "\n",
    "- Function name: top_ten_features.\n",
    "\n",
    "    - Input: corpus (X_arr).\n",
    "    - Output: A list of tuples where the first tuple item is the word and the second tuple item is the frequency. Call this list 'top_ten'. \n",
    "\n",
    "**Note**: Use the same parameters as above for Countvectoriser.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5.2\n",
    "points: \n",
    "    each: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 10152), ('s', 5958), ('movie', 4388), ('film', 3936), ('t', 3385), ('like', 1932), ('just', 1761), ('good', 1461), ('time', 1167), ('story', 1141), ('really', 1128), ('bad', 906), ('people', 876), ('great', 867), ('don', 852)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def top_15_features(corpus):\n",
    "    vec =CountVectorizer(stop_words = 'english', token_pattern =r'\\b[^\\d\\W]+\\b', max_features = 1000)\n",
    "    BoW = vec.fit_transform(corpus)\n",
    "    unique_words = vec.get_feature_names()\n",
    "    sum_words = BoW.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return words_freq[:15]\n",
    "\n",
    "corpus =X_arr\n",
    "top_ten = top_15_features(corpus)\n",
    "\n",
    "print(top_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5.2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5.2 results: All test cases passed!"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 5.3 [2]\n",
    "\n",
    "Call the function below to display the top ten words of each topic. As input, the function takes:\n",
    "\n",
    "- the fitted model.\n",
    "- the feature_names.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5\n",
    "manual: true\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film t lot expect s feature time plot short watching\n",
      "Topic 1:\n",
      "period little piece worth br tough soundtrack modern watch true\n",
      "Topic 2:\n",
      "king earth star father hold br adult plain focus sequel\n",
      "Topic 3:\n",
      "plot car thriller turns special type star given actor effective\n",
      "Topic 4:\n",
      "br series episode match fi sci episodes season tv effects\n",
      "Topic 5:\n",
      "horror film gore films zombie dead sequel fans genre classic\n",
      "Topic 6:\n",
      "film funny original just movie high like s characters school\n",
      "Topic 7:\n",
      "video cult ultimately motion brought sequence baby opening set excellent\n",
      "Topic 8:\n",
      "mainly stop kept older beginning screenplay understand violent needs fight\n",
      "Topic 9:\n",
      "game br jimmy island prison s western drugs army gun\n",
      "Topic 10:\n",
      "movie t just br bad good s like don really\n",
      "Topic 11:\n",
      "dr disney cat seven s terrific jack cast town lady\n",
      "Topic 12:\n",
      "girls sex s women school woman kids like pretty girl\n",
      "Topic 13:\n",
      "br s film movie t like just story good time\n",
      "Topic 14:\n",
      "ii pure entertainment basically eyes just audience fighting gore film\n",
      "Topic 15:\n",
      "doctor hospital father movie george mad indian great job night\n",
      "Topic 16:\n",
      "war cold british german battle political century filled production large\n",
      "Topic 17:\n",
      "japanese henry wife train tom version br ghost s kill\n",
      "Topic 18:\n",
      "br lot possible came saw monster trying year horror film\n",
      "Topic 19:\n",
      "s movie funny things story just good thing added better\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "\n",
    "display_topics(lda,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <p>Your submission has been exported. Click <a href=\"assignment_6_2021_11_10T19_48_57_834142.zip\" target=\"_blank\">here</a>\n",
       "                to download the zip file.</p>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
